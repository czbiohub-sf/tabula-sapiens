{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook lets you annotate your own data with the tabula sapiens dataset.\n",
    "\n",
    "By default, it will run the following methods: `onclass`, `scanVI`, `svm`, and `singleCellNet`. Compute permitting, we suggest running all the methods. If your dataset exceeds 100k cells, total runtime will be around 2-3 hours on GPU. (CHECK THIS)\n",
    "\n",
    "\n",
    "## Arguments:\n",
    "- **annotation_method**: list from [`\"onclass\"`, `\"scanvi\"`, `\"svm\"`, `\"singlecellnet\"`]\n",
    "- **tissue**: `None` or one of [`Bladder`, `Blood`, `Bone_Marrow`, `Kidney`, `Large_Intestine`, `Lung`, `Lymph_Node`, `Muscle`, `Pancreas`, `Skin`,`Small_Intestine`, `Spleen`, `Thymus`, `Trachea`, `Vasculature`]. If `None`, will use the entire tabula sapiens dataset.\n",
    "- **input_anndata**: path to your input anndata\n",
    "- **output_folder_name**: folder in `/data` to save outputs to. Should be unexisting directory\n",
    "- **use_gpu**: if `True`, will use the GPU for training. Note: runtimes are significantly longer on CPU\n",
    "- **use_10X_only**: If `True`, only uses the 10X data from tabula sapiens. This should only equal True if `input_anndata` is 10X. Based on our observations, scanVI will perform better if only using 10X data. Should not be True if you have smartseq2 data.\n",
    "- **batch_correction_conditions**: List from [`\"donor\"`, `\"method\"`] or `None`\n",
    "\n",
    "Optional arguments for scanVI:\n",
    "- **scvi_model**: path to pretrained scvi model. Default: `None`.\n",
    "- **scanvi_model**: path to pretrained scanvi mode. Default: `None`.\n",
    "- **n_scvi_epochs**: n_epochs to train scvi for. Default: `400` \n",
    "- **n_scanvi_epochs**: n_epochs to train scanvi for. Default: `15`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from annotation import setup_dataset, svm_annotation, onclass_annotation, singlecellnet_annotation\n",
    "\n",
    "# annotation_method = [\"onclass\", 'scanvi', 'svm']\n",
    "annotation_method = ['onclass']\n",
    "\n",
    "tissue = 'Lung'\n",
    "input_anndata = 'data/Lung_test.h5ad'\n",
    "output_folder_name = 'lung_evaluation'\n",
    "use_gpu = True\n",
    "use_10X_only= True\n",
    "batch_correction_conditions = ['donor', 'method']\n",
    "annotation_key = 'manual_annotation'\n",
    "\n",
    "#refernce dataset filepath\n",
    "tabula_sapiens_filepath = 'data/Lung_ref.h5ad'\n",
    "\n",
    "#scVI arguments:\n",
    "scvi_model = None\n",
    "scanvi_model= None\n",
    "n_scvi_epochs= 400\n",
    "n_scanvi_epochs = 15\n",
    "\n",
    "output_folder = os.path.join('data', output_folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "data/lung_evaluation already exists. Please provide an unexisting directory to save outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-13696e038b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} already exists. Please provide an unexisting directory to save outputs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: data/lung_evaluation already exists. Please provide an unexisting directory to save outputs"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "else:\n",
    "    raise ValueError(\"{} already exists. Please provide an unexisting directory to save outputs\".format(output_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "Trying to set attribute `.obs` of view, copying.\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:144: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
      "  df.loc[: int(n_top_genes), 'highly_variable'] = True\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 98366 × 4000\n",
      "    obs: 'cell_id', 'method', 'donor', 'manual_annotation', 'donor_method', 'tissue', '_batch', '_batch_indices', 'batch', '_dataset'\n",
      "    var: 'feature_types.0.0-0', 'n_cells.0.0-0', 'gene_symbol.0.0-0', 'n_cells.1.0-0', 'n_cells.0-0', 'n_cells.1.1-0', 'feature_types.0.0.0.1-0', 'gene_symbol.0.0.0.1-0', 'n_cells.1.0.0.1-0', 'n_cells.1.0.1-0', 'n_cells-0', 'len-0', 'ensembl_id-0', 'contamination_prop-0-0', 'contamination_prop-1-0', 'contamination_prop-10-0', 'contamination_prop-11-0', 'contamination_prop-12-0', 'contamination_prop-13-0', 'contamination_prop-14-0', 'contamination_prop-2-0', 'contamination_prop-3-0', 'contamination_prop-4-0', 'contamination_prop-5-0', 'contamination_prop-6-0', 'contamination_prop-7-0', 'contamination_prop-8-0', 'contamination_prop-9-0', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
      "    uns: 'hvg'\n",
      "Embed the cell ontology\n",
      "init OnClass\n",
      "Here, we used the pretrain cell type embedding file tp2emb_500\n",
      "100.000000 precentage of labels are in the Cell Ontology\n",
      "Loading the new dataset.\n",
      "no label file is provided\n",
      "Predicting the labels of cells\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from data/OnClass_data/pretrain/BilinearNN_50019\n",
      "training finished\n",
      "number of intersection genes 15\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chenling_cl.obo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c447f1c5ae24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                        \u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                        \u001b[0mref_anndata_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'data/OnClass_data/data_used_for_training/tabula-muris-senis-facs_cell_ontology_test.h5ad'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                        ref_label_key = 'manual_cell_ontology_class')\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"svm\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotation_method\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/annotation.py\u001b[0m in \u001b[0;36monclass_annotation\u001b[0;34m(input_anndata, output_folder, ref_anndata_path, ref_label_key)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# Add the new annotations to the working file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_anndata_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_AnnData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2tp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_mapping_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_mapping_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#'../data/OnClass_data/cell_ontology/cl.obo')#output_file is optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0monclass_output_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onclass_annotations.h5ad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/OnClass/utils.py\u001b[0m in \u001b[0;36mwrite_anndata_data\u001b[0;34m(test_label, test_AnnData, i2tp, name_mapping_file)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mco2name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2co\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ontology_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_mapping_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_AnnData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mncell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/OnClass/utils.py\u001b[0m in \u001b[0;36mget_ontology_name\u001b[0;34m(cell_type_name_file)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_ontology_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_type_name_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_type_name_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mco2name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mname2co\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chenling_cl.obo'"
     ]
    }
   ],
   "source": [
    "full = setup_dataset(input_anndata, tabula_sapiens_filepath, tissue, use_10X_only, batch_correction_conditions)\n",
    "\n",
    "import scanpy as sc\n",
    "sc.pp.highly_variable_genes(full, flavor='seurat_v3', subset=True, n_top_genes=4000)\n",
    "print(full)\n",
    "\n",
    "if 'scanvi' in annotation_method:\n",
    "    scanvi_annotation(\n",
    "        full_dataset= full,\n",
    "        batch_key='batch_indices', \n",
    "        output_folder = output_folder,\n",
    "        ts_label_key = annotation_key,\n",
    "        scvi_model=None,\n",
    "        scanvi_model=None,\n",
    "        n_scvi_epochs = 72,\n",
    "        n_scanvi_epochs = 5,\n",
    "        use_gpu = use_gpu)\n",
    "    \n",
    "if \"onclass\" in annotation_method:\n",
    "    onclass_annotation(input_anndata, \n",
    "                       output_folder, \n",
    "                       ref_anndata_path= 'data/OnClass_data/data_used_for_training/tabula-muris-senis-facs_cell_ontology_test.h5ad',\n",
    "                       ref_label_key = 'manual_cell_ontology_class')\n",
    "\n",
    "if \"svm\" in annotation_method:\n",
    "    svm_annotation( \n",
    "    full,\n",
    "    batch_key='_batch_indices', \n",
    "    ts_label_key = annotation_key,\n",
    "    output_folder= output_folder)\n",
    "\n",
    "if \"singlecellnet\" in annotation_method:\n",
    "    singlecellnet_annotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"onclass\" in annotation_method:\n",
    "    onclass_annotation(input_anndata, \n",
    "                       output_folder, \n",
    "                       ref_anndata_path ='data/Lung_ref.h5ad',\n",
    "                       ref_label_key = 'manual_annotation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manual_annotation'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_dataset(train_data, labels_key, n_samples):\n",
    "    sample_idx = []\n",
    "    labels, counts = np.unique(train_data.obs[labels_key], return_counts=True)\n",
    "    for i, label in enumerate(labels):\n",
    "        label_locs = np.where(train_data.obs[labels_key] == label)[0]        \n",
    "        if counts[i] < n_samples:\n",
    "            sample_idx.append(label_locs)\n",
    "        else:\n",
    "            label_subset = np.random.choice(label_locs, n_samples, replace = False)\n",
    "            sample_idx.append(label_subset)\n",
    "    sample_idx = np.concatenate(sample_idx)\n",
    "    return sample_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = full_dataset[full_dataset.obs['_dataset'] == 'tabula_sapiens']\n",
    "train_idx = subsample_dataset(train_data, annotation_key, 100)\n",
    "train_data = train_data[train_idx].copy()\n",
    "test_data = full_dataset[full_dataset.obs['_dataset'] == 'user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3826 × 4000\n",
       "    obs: 'cell_id', 'method', 'donor', 'manual_annotation', 'donor_method', 'tissue', '_batch', '_batch_indices', 'batch', '_dataset'\n",
       "    var: 'feature_types.0.0-0', 'n_cells.0.0-0', 'gene_symbol.0.0-0', 'n_cells.1.0-0', 'n_cells.0-0', 'n_cells.1.1-0', 'feature_types.0.0.0.1-0', 'gene_symbol.0.0.0.1-0', 'n_cells.1.0.0.1-0', 'n_cells.1.0.1-0', 'n_cells-0', 'len-0', 'ensembl_id-0', 'contamination_prop-0-0', 'contamination_prop-1-0', 'contamination_prop-10-0', 'contamination_prop-11-0', 'contamination_prop-12-0', 'contamination_prop-13-0', 'contamination_prop-14-0', 'contamination_prop-2-0', 'contamination_prop-3-0', 'contamination_prop-4-0', 'contamination_prop-5-0', 'contamination_prop-6-0', 'contamination_prop-7-0', 'contamination_prop-8-0', 'contamination_prop-9-0', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n",
       "    uns: 'hvg', 'log1p'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data.X\n",
    "test_X = test_data.X\n",
    "train_Y = train_data.obs[annotation_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index = test_data.obs_names, data = test_Y).to_csv('data/lung_evaluation/random_forest_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/annotation.py(203)svm_annotation()\n",
      "-> train_data = train_data[train_idx].copy()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/annotation.py(204)svm_annotation()\n",
      "-> print(train_data)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 3826 × 4000\n",
      "    obs: 'cell_id', 'method', 'donor', 'manual_annotation', 'donor_method', 'tissue', '_batch', '_batch_indices', 'batch', '_dataset'\n",
      "    var: 'feature_types.0.0-0', 'n_cells.0.0-0', 'gene_symbol.0.0-0', 'n_cells.1.0-0', 'n_cells.0-0', 'n_cells.1.1-0', 'feature_types.0.0.0.1-0', 'gene_symbol.0.0.0.1-0', 'n_cells.1.0.0.1-0', 'n_cells.1.0.1-0', 'n_cells-0', 'len-0', 'ensembl_id-0', 'contamination_prop-0-0', 'contamination_prop-1-0', 'contamination_prop-10-0', 'contamination_prop-11-0', 'contamination_prop-12-0', 'contamination_prop-13-0', 'contamination_prop-14-0', 'contamination_prop-2-0', 'contamination_prop-3-0', 'contamination_prop-4-0', 'contamination_prop-5-0', 'contamination_prop-6-0', 'contamination_prop-7-0', 'contamination_prop-8-0', 'contamination_prop-9-0', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n",
      "    uns: 'hvg', 'log1p'\n",
      "> /home/annotation.py(205)svm_annotation()\n",
      "-> print('suhh')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suhh\n",
      "> /home/annotation.py(206)svm_annotation()\n",
      "-> test_data = full_dataset[full_dataset.obs['_dataset'] == 'user']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n",
      "WARNING: adata.X seems to be already log-transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py:339: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n",
      "... storing 'SVM_pred' as categorical\n"
     ]
    }
   ],
   "source": [
    "if \"svm\" in annotation_method:\n",
    "    svm_annotation( \n",
    "    full,\n",
    "    batch_key='_batch_indices', \n",
    "    ts_label_key = annotation_key,\n",
    "    output_folder= output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it took 6:30 for unsupervised training of scvi\n",
    "# it took 5:30 for semi supervised training of scanvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "Trying to set attribute `.obs` of view, copying.\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py:144: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
      "  df.loc[: int(n_top_genes), 'highly_variable'] = True\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n"
     ]
    }
   ],
   "source": [
    "import scvi\n",
    "full = setup_dataset(input_anndata, tabula_sapiens_filepath, tissue, use_10X_only, batch_correction_conditions)\n",
    "sc.pp.highly_variable_genes(full, flavor='seurat_v3', subset=True, n_top_genes=4000)\n",
    "\n",
    "train_data = full[full.obs['_dataset'] == 'tabula_sapiens'].copy()\n",
    "test_data = full[full.obs['_dataset'] == 'user'].copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ..., 68., 76., 83.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data.X[:100].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def subsample_dataset(train_data, labels_key, n_samples):\n",
    "    sample_idx = []\n",
    "    labels, counts = np.unique(train_data.obs[labels_key], return_counts=True)\n",
    "    for i, label in enumerate(labels):\n",
    "        label_locs = np.where(train_data.obs[labels_key] == label)[0]        \n",
    "        if counts[i] < n_samples:\n",
    "            sample_idx.append(label_locs)\n",
    "        else:\n",
    "            label_subset = np.random.choice(label_locs, n_samples, replace = False)\n",
    "            sample_idx.append(label_subset)\n",
    "    sample_idx = np.concatenate(sample_idx)\n",
    "    return sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = subsample_dataset(train_data, labels_key = 'manual_annotation', n_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train_data.obs['scanvi_labels'] = 'unknown'\n",
    "train_data.obs['scanvi_labels'][sample_idx] = train_data.obs['manual_annotation'][sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO\u001b[0m      Using batches from adata.obs\u001b[1m[\u001b[0m\u001b[32m\"donor_method\"\u001b[0m\u001b[1m]\u001b[0m                                       \n",
      "\u001b[34mINFO\u001b[0m      Using labels from adata.obs\u001b[1m[\u001b[0m\u001b[32m\"scanvi_labels\"\u001b[0m\u001b[1m]\u001b[0m                                       \n",
      "\u001b[34mINFO\u001b[0m      Using data from adata.X                                                            \n",
      "\u001b[34mINFO\u001b[0m      Computing library size prior per batch                                             \n",
      "\u001b[34mINFO\u001b[0m      Successfully registered anndata object containing \u001b[1;34m65662\u001b[0m cells, \u001b[1;34m4000\u001b[0m vars, \u001b[1;34m3\u001b[0m        \n",
      "          batches, \u001b[1;34m43\u001b[0m labels, and \u001b[1;34m0\u001b[0m proteins. Also registered \u001b[1;34m0\u001b[0m extra categorical covariates \n",
      "          and \u001b[1;34m0\u001b[0m extra continuous covariates.                                                 \n",
      "\u001b[34mINFO\u001b[0m      Please do not further modify adata until model is trained.                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n"
     ]
    }
   ],
   "source": [
    "scvi.data.setup_anndata(train_data, batch_key = 'donor_method', labels_key = 'scanvi_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scvi.model.SCANVI(train_data,\n",
    "                          unlabeled_category = 'unknown', \n",
    "                          use_cuda = True, \n",
    "                          n_layers=3, \n",
    "                          n_latent=50, \n",
    "                          dispersion='gene-batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_trainer_kwargs = dict(data_loader_kwargs=dict(pin_memory=True))\n",
    "semisupervised_trainer_kwargs = dict(data_loader_kwargs=dict(pin_memory=True))\n",
    "semisupervised_train_kwargs = dict(batch_size = 1024)\n",
    "unsupervised_train_kwargs = dict(batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO\u001b[0m      Training Unsupervised Trainer for \u001b[1;34m122\u001b[0m epochs.                                      \n",
      "\u001b[34mINFO\u001b[0m      Training SemiSupervised Trainer for \u001b[1;34m10\u001b[0m epochs.                                     \n",
      "\u001b[34mINFO\u001b[0m      KL warmup for \u001b[1;34m10\u001b[0m epochs                                                            \n",
      "Training...:   1%|          | 1/122 [04:50<9:46:37, 290.89s/it]\n",
      "Training...:   4%|▍         | 5/122 [19:05<7:23:41, 227.54s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-4b52c5a46cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0msemisupervised_trainer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemisupervised_trainer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0msemisupervised_train_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msemisupervised_train_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             unsupervised_train_kwargs = unsupervised_train_kwargs)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/model/scanvi.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs_unsupervised, n_epochs_semisupervised, train_size, test_size, lr, n_epochs_kl_warmup, n_iter_kl_warmup, frequency, unsupervised_trainer_kwargs, semisupervised_trainer_kwargs, unsupervised_train_kwargs, semisupervised_train_kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             )\n\u001b[1;32m    244\u001b[0m             self._unsupervised_trainer.train(\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs_unsupervised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munsupervised_train_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsupervised_history_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unsupervised_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, lr, eps, params, **extras_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iteration_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# Update the model's parameters after seeing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Checks the training status, ensures no nan loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iteration_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/trainers/trainer.py\u001b[0m in \u001b[0;36mon_training_loop\u001b[0;34m(self, tensors_dict)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/trainers/inference.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, tensors, feed_labels)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         reconst_loss, kl_divergence_local, kl_divergence_global = self.model(\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0msample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_l_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_l_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m         loss = (\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/modules/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, local_l_mean, local_l_var, batch_index, y)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mkl_divergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_divergence_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mreconst_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpx_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreconst_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl_divergence_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/modules/vae.py\u001b[0m in \u001b[0;36mget_reconstruction_loss\u001b[0;34m(self, x, px_rate, px_r, px_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                     \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpx_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpx_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzi_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpx_dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 )\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/distributions/_negative_binomial.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_zinb_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzi_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tabula_sapiens/lib/python3.6/site-packages/scvi/core/distributions/_negative_binomial.py\u001b[0m in \u001b[0;36mlog_zinb_positive\u001b[0;34m(x, mu, theta, pi, eps)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )  # In this case, we reshape theta for broadcasting\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msoftplus_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  uses log(sigmoid(x)) = -softplus(-x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mlog_theta_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlog_theta_mu_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(train_size=1.0,n_epochs_kl_warmup=10,unsupervised_trainer_kwargs=unsupervised_trainer_kwargs ,\n",
    "            semisupervised_trainer_kwargs = semisupervised_trainer_kwargs,\n",
    "            semisupervised_train_kwargs=semisupervised_train_kwargs,\n",
    "            unsupervised_train_kwargs = unsupervised_train_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "full = setup_dataset(input_anndata, tabula_sapiens_filepath, tissue, use_10X_only, batch_correction_conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<102665x4000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 24318139 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['user'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(full.obs['_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Lung_test.h5ad'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Lung_ref.h5ad'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabula_sapiens_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabula_sapiens",
   "language": "python",
   "name": "tabula_sapiens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
